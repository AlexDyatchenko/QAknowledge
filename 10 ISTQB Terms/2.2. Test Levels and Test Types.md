# Test Levels and Test Types

**Test levels** = **groups of test activities that are organized and managed together**. Each test level is an instance of the test process, performed in relation to software at a given phase of development, from 
- individual components to 
- complete systems or, where applicable, 
- systems of systems.

Test levels are related to other activities within the SDLC. In sequential SDLC models, the test levels are often defined such that 
- the exit criteria of one level are part of the entry criteria for the next level. 
In some iterative models, this may not apply. Development activities may span through multiple test levels. 
- Test levels may overlap in time.

**Test types** = **groups of test activities related to specific quality characteristics** and most of those test activities can be performed at every test level.

    TEST LEVELS (What & When)       TEST TYPES (How & What Aspect)
    ├─ Unit Testing                 ├─ Functional Testing
    ├─ Integration Testing          ├─ Non-Functional Testing
    ├─ System Testing               │  ├─ Performance
    └─ Acceptance Testing           │  ├─ Security
                                    │  ├─ Usability
                                    │  └─ Compatibility
                                    ├─ Structural Testing
                                    ├─ Change-Related Testing
                                    └─ Experience-Based Testing

Key difference:
Test Levels answer: "At what stage and scope do we test?"
Test Types answer : "What aspect/characteristic do we test for?"

## Test Levels

In this syllabus, the following five test levels are described:
- **Component testing** = **unit testing** focuses on testing components in isolation. It often requires specific support, such as test harnesses or unit test frameworks. Component testing is normally **performed by developers** in their development environments.
- **Component integration testing** = **unit integration testing** focuses on testing the interfaces and interactions between components. Component integration testing is heavily dependent on the integration strategy like bottom-up, top-down or big-bang.
- **System testing** focuses on the overall behavior and capabilities of an **entire system** or product, often including:
    - functional testing of end-to-end tasks and
    - non-functional testing of quality characteristics.
        - For some non-functional quality characteristics, it is preferable to test them on a complete system in a representative test environment (e.g., usability). Using simulations of sub-systems is also possible.  
    - often performed by an **independent test team**, and is related to specifications for the system.
- **System integration** testing focuses on testing the interfaces of the system under test and other systems and external services. System integration testing requires suitable test environments preferably similar to the operational environment.
- **Acceptance testing** focuses on validation and on demonstrating readiness for deployment, which means that the system fulfills the user’s business needs. Ideally, acceptance testing should be performed by the intended users. The main forms of acceptance testing are: 
    - user acceptance testing (UAT), 
    - operational acceptance testing, 
    - contractual acceptance testing and 
    - regulatory acceptance testing, 
    - alpha testing and 
    - beta testing.
        -  Beta testing is a type of acceptance testing performed atan external site by roles outside the development organization

Test levels are distinguished by the following non-exhaustive list of attributes, to avoid overlapping of test activities:
- Test object
- Test objectives
- Test basis
- Defects and failures
- Approach and responsibilities

### Test Types

A lot of test types exist and can be applied in projects. In this syllabus, the following four test types are addressed:
- **Functional testing** evaluates the functions that a component or system should perform. The **functions are “what” the test object should do**. The main objective of functional testing is checking the functional completeness, functional correctness and functional appropriateness.
- **Non-functional testing** evaluates attributes **other than functional characteristics** of a component or system. Non-functional testing is the testing of “how well the system behaves”. The main objective of non-
functional testing is checking the non-functional quality characteristics.  Сlassification of the non-functional quality characteristics:
- Performance efficiency
- Compatibility
- Usability (also known as interaction capability)
- Reliability
- Security
- Maintainability
- Portability (also known as flexibility)
- Safety

It is sometimes appropriate for non-functional testing to start early in the SDLC (e.g., as part of reviews or component testing). Many non-functional tests are 
- derived from functional tests as they use the same functional tests, but check that while performing the function, a non-functional constraint is satisfied (e.g., 
    - checking that a function performs within a specified time, or a function can be ported to a new platform).

The late discovery of non-functional defects can pose a serious threat to the success of a project. Non-functional testing sometimes needs a very specific test environment, such as a 
- usability lab for usability testing.

**Black-box testing** is specification-based and derives tests from documentation not related to the internal structure of the test object. 
- main objective is checking the system's **behavior** against its specifications.

**White-box testing** is 
- structure-based and 
- derives tests from the system's implementation or 
- internal structure (e.g., 
    - code, 
    - architecture, 
    - work flows, and 
    - data flows). 
    
- main objective is to cover the underlying structure by the tests to an **acceptable level**.
All the four above mentioned test types can be 
- applied to **all test levels**, although the 
- focus will be different at each level. 
- Different test techniques can be used 
    - to derive test conditions and test cases for all the mentioned test types.

## Confirmation Testing and Regression Testing

Changes are typically made to a component or system to either enhance it by 
- adding a new feature or to 
- fix it by removing a defect. 

Testing should then also include 
- confirmation testing and 
- regression testing.

**Confirmation testing** confirms that an **original defect has been fixed**. Depending on the risk, one can test the fixed version of the software in several ways, including:
- executing all tests that previously have failed due to the defect, or, also by
- adding new tests to cover any changes that were needed to fix the defect

However, when time or money is short when fixing defects, confirmation testing might be restricted to simply exercising the test steps that should reproduce the failure caused by the defect and checking that the failure does not occur.

**Regression testing** confirms that no adverse consequences have been caused by a change, including a fix that has already been confirmation tested. These adverse consequences could affect the 
- same component where the change was made, 
- other components in the same system, or even 
- other connected systems. 
- can be related to the environment. 

It is advisable first to **perform an impact analysis** to recognize the extent of the regression testing that shows which parts of the software could be affected.
Regression test suites are 
- run many times and generally the number of regression test cases will increase with each iteration or release, 
- strong candidate for automation. 
- Test automation should start early in the project. 

Where CI is used, such as in [DevOps](/10%20ISTQB%20Terms/2.1%20Testing%20in%20the%20Context%20of%20a%20Software%20Development%20Lifecycle.md#devops), it is good practice to also include automated regression tests. Depending on the situation, this may include
regression tests on different test levels.
Confirmation testing and/or regression testing for the test object are needed on all test levels if defects
are fixed and/or changes are made on these test levels.

##  Maintenance Testing

There are different categories of maintenance, it can be 
- corrective, 
- adaptive to changes in theenvironment or 
- improve performance or 
- improve maintainability, 

so maintenance can involve 
- planned releases/deployments and 
- unplanned releases/deployments (hot fixes). 

Impact analysis may be done *before* a change is made, to help decide if the change should be made, based on the potential consequences in other areas of the system. 
Testing the changes to an operational system includes both 
- evaluating the success of the implementation of the change and the 
- checking for possible regressions in parts of the system that remain unchanged (which is usually most of the system).

The scope of maintenance testing typically depends on:
- The degree of risk of the change
- The size of the existing system
- The size of the change

The triggers for maintenance and maintenance testing can be classified as follows:
- Modifications, such as planned enhancements (i.e., 
    - release-based), 
    - corrective changes or 
    - hot fixes.
- Upgrades or migrations of the operational environment, such as from one platform to another, which can require tests associated with the 
    - new environment as well as of the 
    - changed software, or 
    - tests of data conversion when data from another application is migrated into the system being maintained.    
- Retirement, such as when an application reaches the end of its life. 

When a system is retired, this can require testing of data archiving if long data retention periods are required. Testing of restore and retrieval procedures after archiving may also be needed in the event that certain data is required during the archiving period.